# Does processing in Canine Vision Spectrum retain performance

One  of  the  most  fundamental  applications  in  the  field  of computer  vision  is  image  classification. Since the advent of deep learning techniques, it has been a norm to resort to RGB color space for processing and forward implementations. In this paper, we aim to find out how processing in the canine vision spectrum affects image classification models considering animals view the world differently than humans and might be able to perceive notions and surrounding objects in a more enhanced/restricted way. Specifically, we benchmark our dataset on various models.

Report: Report.pdf

Video Link: https://youtu.be/HO7483F-RbA

Slides: Slides.pdf

## Installation

#### First clone the repository using

```http
  git clone https://github.com/EeshaanJain/EE610-Project
```
#### Now to install the required python packages

```http
  pip install -r requirements.txt
```

## Usage
The file `project.py` can be used to run the inference of your image on any of the models using the canine spectrum. Running 
```bash
python project.py --help 
```
gives
```
usage: EE 610 Project [-h] [--image i] [--convert c] [--model m] [--mode md]

optional arguments:
  -h, --help   show this help message and exit
  --image i    Takes in path to input image file
  --convert c  Converts image to canine vision space (default is True)
  --model m    Enter either of
                       [1] d   : DenseNet
                       [2] r   : ResNet
                       [3] e   : EfficientNet
  --mode md    Choose if you want the Lp mode or Sp mode. Default is Lp
```
1. The image parameter is used to take the input path of the image
2. The convert parameter is used to convert the human spectrum image to the canine spectrum image. It is by default True, since usually we don't have access to such images. But if the image is already in the canine spectrum, set it to False.
3. The model parameter is used to choose which model you want to test the image on. By default it is set to DenseNet i.e 'd'
4. The mode parameter allows you to either choose the high parameter mode, or the low parameter mode. By default it is set to Lp, but choose Sp if you want low parameter mode.

An example test case is:
```bash
python project.py --image test-cat.jpg --convert True --model r --mode Lp 
```

This gives the output as shown. (Note that the image on top-left is the original one, and one on the top-right is the canine vision one. Also, the canine vision image is stored in the temp directory created.)
![slide 21](example.png)

## Directory structure
- ðŸ“‚ __EE610\-Project__
   - ðŸ“„ [DenseNet\-LMS\-less.pth](DenseNet-LMS-less.pth)
   - ðŸ“„ [DenseNet\-LMS.pth](DenseNet-LMS.pth)
   - ðŸ“„ [DenseNet\-RGB\-less.pth](DenseNet-RGB-less.pth)
   - ðŸ“„ [DenseNet\-RGB.pth](DenseNet-RGB.pth)
   - ðŸ“„ [EfficientNet\-LMS\-less.pth](EfficientNet-LMS-less.pth)
   - ðŸ“„ [EfficientNet\-LMS.pth](EfficientNet-LMS.pth)
   - ðŸ“„ [EfficientNet\-RGB\-less.pth](EfficientNet-RGB-less.pth)
   - ðŸ“„ [EfficientNet\-RGB.log](EfficientNet-RGB.log)
   - ðŸ“„ [EfficientNet\-RGB.pth](EfficientNet-RGB.pth)
   - ðŸ“„ [README.md](README.md)
   - ðŸ“„ [Report.pdf](Report.pdf)
   - ðŸ“„ [Resnet\-LMS\-less.pth](Resnet-LMS-less.pth)
   - ðŸ“„ [Resnet\-LMS.pth](Resnet-LMS.pth)
   - ðŸ“„ [Resnet\-RGB\-less.pth](Resnet-RGB-less.pth)
   - ðŸ“„ [Resnet\-RGB.pth](Resnet-RGB.pth)
   - ðŸ“„ [Slides.pdf](Slides.pdf)
   - ðŸ“„ [dataloader.py](dataloader.py)
   - ðŸ“„ [densenet.py](densenet.py)
   - ðŸ“„ [effnet.py](effnet.py)
   - ðŸ“„ [env.bat](env.bat)
   - ðŸ“„ [example.png](example.png)
   - ðŸ“„ [extracting\_data.py](extracting_data.py)
   - ðŸ“„ [help.txt](help.txt)
   - ðŸ“„ [image\_converter.py](image_converter.py)
   - ðŸ“‚ __models__
     - ðŸ“‚ __densenet__
       - ðŸ“„ [DenseNet\-LMS\-less.log](models/densenet/DenseNet-LMS-less.log)
       - ðŸ“„ [DenseNet\-LMS\-less.png](models/densenet/DenseNet-LMS-less.png)
       - ðŸ“„ [DenseNet\-LMS.log](models/densenet/DenseNet-LMS.log)
       - ðŸ“„ [DenseNet\-LMS.png](models/densenet/DenseNet-LMS.png)
       - ðŸ“„ [DenseNet\-RGB\-less.log](models/densenet/DenseNet-RGB-less.log)
       - ðŸ“„ [DenseNet\-RGB\-less.png](models/densenet/DenseNet-RGB-less.png)
       - ðŸ“„ [DenseNet\-RGB.log](models/densenet/DenseNet-RGB.log)
       - ðŸ“„ [DenseNet\-RGB.png](models/densenet/DenseNet-RGB.png)
     - ðŸ“‚ __effnet__
       - ðŸ“„ [EfficientNet\-LMS\-less.log](models/effnet/EfficientNet-LMS-less.log)
       - ðŸ“„ [EfficientNet\-LMS\-less.png](models/effnet/EfficientNet-LMS-less.png)
       - ðŸ“„ [EfficientNet\-LMS.log](models/effnet/EfficientNet-LMS.log)
       - ðŸ“„ [EfficientNet\-LMS.png](models/effnet/EfficientNet-LMS.png)
       - ðŸ“„ [EfficientNet\-RGB\-less.log](models/effnet/EfficientNet-RGB-less.log)
       - ðŸ“„ [EfficientNet\-RGB\-less.png](models/effnet/EfficientNet-RGB-less.png)
       - ðŸ“„ [EfficientNet\-RGB.png](models/effnet/EfficientNet-RGB.png)
     - ðŸ“‚ __resnet__
       - ðŸ“„ [Resnet\-LMS\-less.log](models/resnet/Resnet-LMS-less.log)
       - ðŸ“„ [Resnet\-LMS\-less.png](models/resnet/Resnet-LMS-less.png)
       - ðŸ“„ [Resnet\-LMS.log](models/resnet/Resnet-LMS.log)
       - ðŸ“„ [Resnet\-LMS.png](models/resnet/Resnet-LMS.png)
       - ðŸ“„ [Resnet\-RGB\-less.log](models/resnet/Resnet-RGB-less.log)
       - ðŸ“„ [Resnet\-RGB\-less.png](models/resnet/Resnet-RGB-less.png)
       - ðŸ“„ [Resnet\-RGB.log](models/resnet/Resnet-RGB.log)
       - ðŸ“„ [Resnet\-RGB.png](models/resnet/Resnet-RGB.png)
   - ðŸ“„ [old.txt](old.txt)
   - ðŸ“„ [project.py](project.py)
   - ðŸ“„ [requirements.txt](requirements.txt)
   - ðŸ“„ [resnet.py](resnet.py)
   - ðŸ“„ [runner.py](runner.py)
   - ðŸ“„ [test\-cat.jpg](test-cat.jpg)
   - ðŸ“„ [test\-dog.jpg](test-dog.jpg)
   - ðŸ“„ [tree.md](tree.md)
   - ðŸ“„ [tree.py](tree.py)
   - ðŸ“„ [tree.txt](tree.txt)

        
## Authors

- Aaryan Gupta
- Eeshaan Jain
- Vipin Singh
